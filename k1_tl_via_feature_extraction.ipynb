{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from imutils import paths\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../train_data'\n",
    "TEST_DATA_PATH = '../train_test_data'\n",
    "\n",
    "CLASSES = ['1', '2', '3', '4', '5']\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features using pre-trained CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VGG16 network and initialize the label encoder\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "le = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train\n",
      "Processing batch 1/176\n",
      "Processing batch 2/176\n",
      "Processing batch 3/176\n",
      "Processing batch 4/176\n",
      "Processing batch 5/176\n",
      "Processing batch 6/176\n",
      "Processing batch 7/176\n",
      "Processing batch 8/176\n",
      "Processing batch 9/176\n",
      "Processing batch 10/176\n",
      "Processing batch 11/176\n",
      "Processing batch 12/176\n",
      "Processing batch 13/176\n",
      "Processing batch 14/176\n",
      "Processing batch 15/176\n",
      "Processing batch 16/176\n",
      "Processing batch 17/176\n",
      "Processing batch 18/176\n",
      "Processing batch 19/176\n",
      "Processing batch 20/176\n",
      "Processing batch 21/176\n",
      "Processing batch 22/176\n",
      "Processing batch 23/176\n",
      "Processing batch 24/176\n",
      "Processing batch 25/176\n",
      "Processing batch 26/176\n",
      "Processing batch 27/176\n",
      "Processing batch 28/176\n",
      "Processing batch 29/176\n",
      "Processing batch 30/176\n",
      "Processing batch 31/176\n",
      "Processing batch 32/176\n",
      "Processing batch 33/176\n",
      "Processing batch 34/176\n",
      "Processing batch 35/176\n",
      "Processing batch 36/176\n",
      "Processing batch 37/176\n",
      "Processing batch 38/176\n",
      "Processing batch 39/176\n",
      "Processing batch 40/176\n",
      "Processing batch 41/176\n",
      "Processing batch 42/176\n",
      "Processing batch 43/176\n",
      "Processing batch 44/176\n",
      "Processing batch 45/176\n",
      "Processing batch 46/176\n",
      "Processing batch 47/176\n",
      "Processing batch 48/176\n",
      "Processing batch 49/176\n",
      "Processing batch 50/176\n",
      "Processing batch 51/176\n",
      "Processing batch 52/176\n",
      "Processing batch 53/176\n",
      "Processing batch 54/176\n",
      "Processing batch 55/176\n",
      "Processing batch 56/176\n",
      "Processing batch 57/176\n",
      "Processing batch 58/176\n",
      "Processing batch 59/176\n",
      "Processing batch 60/176\n",
      "Processing batch 61/176\n",
      "Processing batch 62/176\n",
      "Processing batch 63/176\n",
      "Processing batch 64/176\n",
      "Processing batch 65/176\n",
      "Processing batch 66/176\n",
      "Processing batch 67/176\n",
      "Processing batch 68/176\n",
      "Processing batch 69/176\n",
      "Processing batch 70/176\n",
      "Processing batch 71/176\n",
      "Processing batch 72/176\n",
      "Processing batch 73/176\n",
      "Processing batch 74/176\n",
      "Processing batch 75/176\n",
      "Processing batch 76/176\n",
      "Processing batch 77/176\n",
      "Processing batch 78/176\n",
      "Processing batch 79/176\n",
      "Processing batch 80/176\n",
      "Processing batch 81/176\n",
      "Processing batch 82/176\n",
      "Processing batch 83/176\n",
      "Processing batch 84/176\n",
      "Processing batch 85/176\n",
      "Processing batch 86/176\n",
      "Processing batch 87/176\n",
      "Processing batch 88/176\n",
      "Processing batch 89/176\n",
      "Processing batch 90/176\n",
      "Processing batch 91/176\n",
      "Processing batch 92/176\n",
      "Processing batch 93/176\n",
      "Processing batch 94/176\n",
      "Processing batch 95/176\n",
      "Processing batch 96/176\n",
      "Processing batch 97/176\n",
      "Processing batch 98/176\n",
      "Processing batch 99/176\n",
      "Processing batch 100/176\n",
      "Processing batch 101/176\n",
      "Processing batch 102/176\n",
      "Processing batch 103/176\n",
      "Processing batch 104/176\n",
      "Processing batch 105/176\n",
      "Processing batch 106/176\n",
      "Processing batch 107/176\n",
      "Processing batch 108/176\n",
      "Processing batch 109/176\n",
      "Processing batch 110/176\n",
      "Processing batch 111/176\n",
      "Processing batch 112/176\n",
      "Processing batch 113/176\n",
      "Processing batch 114/176\n",
      "Processing batch 115/176\n",
      "Processing batch 116/176\n",
      "Processing batch 117/176\n",
      "Processing batch 118/176\n",
      "Processing batch 119/176\n",
      "Processing batch 120/176\n",
      "Processing batch 121/176\n",
      "Processing batch 122/176\n",
      "Processing batch 123/176\n",
      "Processing batch 124/176\n",
      "Processing batch 125/176\n",
      "Processing batch 126/176\n",
      "Processing batch 127/176\n",
      "Processing batch 128/176\n",
      "Processing batch 129/176\n",
      "Processing batch 130/176\n",
      "Processing batch 131/176\n",
      "Processing batch 132/176\n",
      "Processing batch 133/176\n",
      "Processing batch 134/176\n",
      "Processing batch 135/176\n",
      "Processing batch 136/176\n",
      "Processing batch 137/176\n",
      "Processing batch 138/176\n",
      "Processing batch 139/176\n",
      "Processing batch 140/176\n",
      "Processing batch 141/176\n",
      "Processing batch 142/176\n",
      "Processing batch 143/176\n",
      "Processing batch 144/176\n",
      "Processing batch 145/176\n",
      "Processing batch 146/176\n",
      "Processing batch 147/176\n",
      "Processing batch 148/176\n",
      "Processing batch 149/176\n",
      "Processing batch 150/176\n",
      "Processing batch 151/176\n",
      "Processing batch 152/176\n",
      "Processing batch 153/176\n",
      "Processing batch 154/176\n",
      "Processing batch 155/176\n",
      "Processing batch 156/176\n",
      "Processing batch 157/176\n",
      "Processing batch 158/176\n",
      "Processing batch 159/176\n",
      "Processing batch 160/176\n",
      "Processing batch 161/176\n",
      "Processing batch 162/176\n",
      "Processing batch 163/176\n",
      "Processing batch 164/176\n",
      "Processing batch 165/176\n",
      "Processing batch 166/176\n",
      "Processing batch 167/176\n",
      "Processing batch 168/176\n",
      "Processing batch 169/176\n",
      "Processing batch 170/176\n",
      "Processing batch 171/176\n",
      "Processing batch 172/176\n",
      "Processing batch 173/176\n",
      "Processing batch 174/176\n",
      "Processing batch 175/176\n",
      "Processing batch 176/176\n",
      "Processing test\n",
      "Processing batch 1/20\n",
      "Processing batch 2/20\n",
      "Processing batch 3/20\n",
      "Processing batch 4/20\n",
      "Processing batch 5/20\n",
      "Processing batch 6/20\n",
      "Processing batch 7/20\n",
      "Processing batch 8/20\n",
      "Processing batch 9/20\n",
      "Processing batch 10/20\n",
      "Processing batch 11/20\n",
      "Processing batch 12/20\n",
      "Processing batch 13/20\n",
      "Processing batch 14/20\n",
      "Processing batch 15/20\n",
      "Processing batch 16/20\n",
      "Processing batch 17/20\n",
      "Processing batch 18/20\n",
      "Processing batch 19/20\n",
      "Processing batch 20/20\n"
     ]
    }
   ],
   "source": [
    "splits = [\n",
    "    ('train', TRAIN_DATA_PATH),\n",
    "    ('test', TEST_DATA_PATH),\n",
    "]\n",
    "\n",
    "# loop over the data splits\n",
    "for split_name,split_path in splits:\n",
    "    print(f'Processing {split_name}')\n",
    "\n",
    "    # grab all image paths in the current split\n",
    "    image_paths = list(paths.list_images(split_path))\n",
    "\n",
    "    # randomly shuffle the image paths and then extract the class labels from the file paths\n",
    "    random.shuffle(image_paths)\n",
    "    labels = [p.split(os.path.sep)[-2] for p in image_paths]\n",
    "\n",
    "    # if the label encoder is None, create it\n",
    "    if le is None:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(labels)\n",
    "\n",
    "    # open the output CSV file for writing\n",
    "    csv_path = f'{split_name}.csv'\n",
    "    csv = open(csv_path, 'w')\n",
    "\n",
    "    # loop over the images in batches\n",
    "    for (b, i) in enumerate(range(0, len(image_paths), BATCH_SIZE)):\n",
    "        # extract the batch of images and labels, then initialize the\n",
    "        # list of actual images that will be passed through the network for feature extraction\n",
    "        print('Processing batch {}/{}'.format(b + 1, int(np.ceil(len(image_paths) / float(BATCH_SIZE)))))\n",
    "        batch_paths = image_paths[i:i + BATCH_SIZE]\n",
    "        batch_labels = le.transform(labels[i:i + BATCH_SIZE])\n",
    "        batch_images = []\n",
    "\n",
    "        # loop over the images and labels in the current batch\n",
    "        for image_path in batch_paths:\n",
    "            # load the input image using the Keras helper utility\n",
    "            # while ensuring the image is resized to 224x224 pixels\n",
    "            image = load_img(image_path, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "\n",
    "            # preprocess the image by (1) expanding the dimensions and\n",
    "            # (2) subtracting the mean RGB pixel intensity from the ImageNet dataset\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "            # add the image to the batch\n",
    "            batch_images.append(image)\n",
    "\n",
    "        # pass the images through the network and use the outputs as\n",
    "        # our actual features, then reshape the features into a flattened volume\n",
    "        batch_images = np.vstack(batch_images)\n",
    "        features = model.predict(batch_images, batch_size=BATCH_SIZE)\n",
    "        features = features.reshape((features.shape[0], 7 * 7 * 512))\n",
    "\n",
    "        # loop over the class labels and extracted features\n",
    "        for (label, vec) in zip(batch_labels, features):\n",
    "            # construct a row that exists of the class label and\n",
    "            # extracted features\n",
    "            vec = ','.join([str(v) for v in vec])\n",
    "            csv.write('{},{}\\n'.format(label, vec))\n",
    "\n",
    "    # close the CSV file\n",
    "    csv.close()\n",
    "\n",
    "# serialize the label encoder to disk\n",
    "f = open('label_encoder.pickle', 'wb')\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading data...\n",
      "[INFO] training model...\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.86      0.85       212\n",
      "           2       0.96      0.97      0.96       116\n",
      "           3       0.96      0.93      0.94        91\n",
      "           4       0.92      0.93      0.92        83\n",
      "           5       0.77      0.71      0.74       121\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       623\n",
      "   macro avg       0.88      0.88      0.88       623\n",
      "weighted avg       0.87      0.87      0.87       623\n",
      "\n",
      "[INFO] saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankwadhwa/Desktop/Work/env/ds3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def load_data_split(splitPath):\n",
    "    # initialize the data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the rows in the data split file\n",
    "    for row in open(splitPath):\n",
    "        # extract the class label and features from the row\n",
    "        row = row.strip().split(',')\n",
    "        label = row[0]\n",
    "        features = np.array(row[1:], dtype='float')\n",
    "\n",
    "        # update the data and label lists\n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "\n",
    "    # convert the data and labels to NumPy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # return a tuple of the data and labels\n",
    "    return (data, labels)\n",
    "\n",
    "# derive the paths to the training and testing CSV files\n",
    "training_path = 'train.csv'\n",
    "testing_path = 'test.csv'\n",
    "\n",
    "# load the data from disk\n",
    "print('[INFO] loading data...')\n",
    "(trainX, trainY) = load_data_split(training_path)\n",
    "(testX, testY) = load_data_split(testing_path)\n",
    "\n",
    "# load the label encoder from disk\n",
    "le = pickle.loads(open('label_encoder.pickle', 'rb').read())\n",
    "\n",
    "# train the model\n",
    "print('[INFO] training model...')\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# evaluate the model\n",
    "print('[INFO] evaluating...')\n",
    "preds = model.predict(testX)\n",
    "print(classification_report(testY, preds, target_names=le.classes_))\n",
    "\n",
    "# serialize the model to disk\n",
    "print('[INFO] saving model...')\n",
    "f = open('lr_model.pickle', 'wb')\n",
    "f.write(pickle.dumps(model))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8708200211344395\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(testY, preds, average='weighted')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
