{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.applications.vgg19 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import activations\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_join(dirname, filenames):\n",
    "    return [os.path.join(dirname, filename) for filename in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and pre-processing\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=[0.9, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory setup\n",
    "\n",
    "train_dir = '../train_data/'\n",
    "test_dir = '../train_test_data/'\n",
    "\n",
    "input_shape = (224, 224)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5629 images belonging to 5 classes.\n",
      "Found 623 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Making data generator\n",
    "generator_train = datagen_train.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "generator_test = datagen_test.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting epoch size for batch step calculation\n",
    "epochs = 50\n",
    "\n",
    "# Steps per epoch setup\n",
    "steps_per_epoch = generator_train.n / batch_size\n",
    "steps_test = generator_test.n / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joing directory and filenames\n",
    "image_paths_train = path_join(train_dir, generator_train.filenames)\n",
    "image_paths_test = path_join(test_dir, generator_test.filenames)\n",
    "\n",
    "# classes available\n",
    "cls_train = generator_train.classes\n",
    "cls_test = generator_test.classes\n",
    "\n",
    "class_names = list(generator_train.class_indices.keys())\n",
    "num_classes = generator_train.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of unequal number of dataset, use class weight balancing\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(cls_train),\n",
    "    y=cls_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 45,720,645\n",
      "Trainable params: 32,775,685\n",
      "Non-trainable params: 12,944,960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_conv = keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting learning rate\n",
    "optimizer = Adam(lr=1e-5)\n",
    "\n",
    "# Loss definition\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "# Metrics for measurement\n",
    "metrics = ['categorical_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4694631928490248610\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11272401716\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11447008020918531338\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "176/175 [==============================] - 90s 511ms/step - loss: 1.1244 - categorical_accuracy: 0.5428 - val_loss: 0.7626 - val_categorical_accuracy: 0.7047\n",
      "Epoch 2/50\n",
      "176/175 [==============================] - 80s 452ms/step - loss: 0.7208 - categorical_accuracy: 0.7173 - val_loss: 0.6879 - val_categorical_accuracy: 0.7368\n",
      "Epoch 3/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.6244 - categorical_accuracy: 0.7475 - val_loss: 0.6502 - val_categorical_accuracy: 0.7657\n",
      "Epoch 4/50\n",
      "176/175 [==============================] - 80s 457ms/step - loss: 0.5727 - categorical_accuracy: 0.7763 - val_loss: 0.5854 - val_categorical_accuracy: 0.7849\n",
      "Epoch 5/50\n",
      "176/175 [==============================] - 82s 467ms/step - loss: 0.5226 - categorical_accuracy: 0.8007 - val_loss: 0.5308 - val_categorical_accuracy: 0.8074\n",
      "Epoch 6/50\n",
      "176/175 [==============================] - 81s 459ms/step - loss: 0.4874 - categorical_accuracy: 0.8135 - val_loss: 0.5361 - val_categorical_accuracy: 0.8010\n",
      "Epoch 7/50\n",
      "176/175 [==============================] - 81s 463ms/step - loss: 0.4702 - categorical_accuracy: 0.8197 - val_loss: 0.5529 - val_categorical_accuracy: 0.8058\n",
      "Epoch 8/50\n",
      "176/175 [==============================] - 82s 463ms/step - loss: 0.4557 - categorical_accuracy: 0.8252 - val_loss: 0.5218 - val_categorical_accuracy: 0.7994\n",
      "Epoch 9/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.4257 - categorical_accuracy: 0.8339 - val_loss: 0.5782 - val_categorical_accuracy: 0.7913\n",
      "Epoch 10/50\n",
      "176/175 [==============================] - 82s 468ms/step - loss: 0.4048 - categorical_accuracy: 0.8406 - val_loss: 0.5987 - val_categorical_accuracy: 0.7705\n",
      "Epoch 11/50\n",
      "176/175 [==============================] - 81s 462ms/step - loss: 0.3870 - categorical_accuracy: 0.8519 - val_loss: 0.5203 - val_categorical_accuracy: 0.8106\n",
      "Epoch 12/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.3898 - categorical_accuracy: 0.8515 - val_loss: 0.4601 - val_categorical_accuracy: 0.8202\n",
      "Epoch 13/50\n",
      "176/175 [==============================] - 82s 469ms/step - loss: 0.3663 - categorical_accuracy: 0.8550 - val_loss: 0.4395 - val_categorical_accuracy: 0.8202\n",
      "Epoch 14/50\n",
      "176/175 [==============================] - 82s 465ms/step - loss: 0.3454 - categorical_accuracy: 0.8657 - val_loss: 0.4280 - val_categorical_accuracy: 0.8427\n",
      "Epoch 15/50\n",
      "176/175 [==============================] - 81s 462ms/step - loss: 0.3406 - categorical_accuracy: 0.8747 - val_loss: 0.4023 - val_categorical_accuracy: 0.8395\n",
      "Epoch 16/50\n",
      "176/175 [==============================] - 82s 466ms/step - loss: 0.3374 - categorical_accuracy: 0.8707 - val_loss: 0.4711 - val_categorical_accuracy: 0.8331\n",
      "Epoch 17/50\n",
      "176/175 [==============================] - 81s 460ms/step - loss: 0.3331 - categorical_accuracy: 0.8723 - val_loss: 0.4386 - val_categorical_accuracy: 0.8363\n",
      "Epoch 18/50\n",
      "176/175 [==============================] - 81s 463ms/step - loss: 0.3202 - categorical_accuracy: 0.8787 - val_loss: 0.3945 - val_categorical_accuracy: 0.8411\n",
      "Epoch 19/50\n",
      "176/175 [==============================] - 82s 465ms/step - loss: 0.3161 - categorical_accuracy: 0.8785 - val_loss: 0.4765 - val_categorical_accuracy: 0.8283\n",
      "Epoch 20/50\n",
      "176/175 [==============================] - 81s 458ms/step - loss: 0.3102 - categorical_accuracy: 0.8816 - val_loss: 0.4821 - val_categorical_accuracy: 0.8250\n",
      "Epoch 21/50\n",
      "176/175 [==============================] - 81s 462ms/step - loss: 0.2960 - categorical_accuracy: 0.8818 - val_loss: 0.4697 - val_categorical_accuracy: 0.8347\n",
      "Epoch 22/50\n",
      "176/175 [==============================] - 82s 463ms/step - loss: 0.2976 - categorical_accuracy: 0.8849 - val_loss: 0.4419 - val_categorical_accuracy: 0.8587\n",
      "Epoch 23/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.2882 - categorical_accuracy: 0.8913 - val_loss: 0.4725 - val_categorical_accuracy: 0.8315\n",
      "Epoch 24/50\n",
      "176/175 [==============================] - 82s 465ms/step - loss: 0.2911 - categorical_accuracy: 0.8927 - val_loss: 0.5028 - val_categorical_accuracy: 0.8218\n",
      "Epoch 25/50\n",
      "176/175 [==============================] - 81s 459ms/step - loss: 0.2752 - categorical_accuracy: 0.8979 - val_loss: 0.5402 - val_categorical_accuracy: 0.8042\n",
      "Epoch 26/50\n",
      "176/175 [==============================] - 82s 467ms/step - loss: 0.2724 - categorical_accuracy: 0.8929 - val_loss: 0.4477 - val_categorical_accuracy: 0.8331\n",
      "Epoch 27/50\n",
      "176/175 [==============================] - 81s 458ms/step - loss: 0.2511 - categorical_accuracy: 0.9062 - val_loss: 0.4251 - val_categorical_accuracy: 0.8475\n",
      "Epoch 28/50\n",
      "176/175 [==============================] - 82s 465ms/step - loss: 0.2493 - categorical_accuracy: 0.8991 - val_loss: 0.4314 - val_categorical_accuracy: 0.8475\n",
      "Epoch 29/50\n",
      "176/175 [==============================] - 81s 460ms/step - loss: 0.2460 - categorical_accuracy: 0.9037 - val_loss: 0.3760 - val_categorical_accuracy: 0.8716\n",
      "Epoch 30/50\n",
      "176/175 [==============================] - 83s 472ms/step - loss: 0.2484 - categorical_accuracy: 0.9016 - val_loss: 0.4664 - val_categorical_accuracy: 0.8363\n",
      "Epoch 31/50\n",
      "176/175 [==============================] - 82s 464ms/step - loss: 0.2379 - categorical_accuracy: 0.9077 - val_loss: 0.4302 - val_categorical_accuracy: 0.8555\n",
      "Epoch 32/50\n",
      "176/175 [==============================] - 83s 469ms/step - loss: 0.2327 - categorical_accuracy: 0.9107 - val_loss: 0.3653 - val_categorical_accuracy: 0.8748\n",
      "Epoch 33/50\n",
      "176/175 [==============================] - 80s 456ms/step - loss: 0.2391 - categorical_accuracy: 0.9052 - val_loss: 0.3727 - val_categorical_accuracy: 0.8716\n",
      "Epoch 34/50\n",
      "176/175 [==============================] - 81s 458ms/step - loss: 0.2308 - categorical_accuracy: 0.9111 - val_loss: 0.4538 - val_categorical_accuracy: 0.8459\n",
      "Epoch 35/50\n",
      "176/175 [==============================] - 80s 457ms/step - loss: 0.2339 - categorical_accuracy: 0.9110 - val_loss: 0.5824 - val_categorical_accuracy: 0.8106\n",
      "Epoch 36/50\n",
      "176/175 [==============================] - 80s 457ms/step - loss: 0.2180 - categorical_accuracy: 0.9168 - val_loss: 0.4300 - val_categorical_accuracy: 0.8539\n",
      "Epoch 37/50\n",
      "176/175 [==============================] - 82s 465ms/step - loss: 0.2182 - categorical_accuracy: 0.9147 - val_loss: 0.3571 - val_categorical_accuracy: 0.8732\n",
      "Epoch 38/50\n",
      "176/175 [==============================] - 82s 464ms/step - loss: 0.2190 - categorical_accuracy: 0.9174 - val_loss: 0.4000 - val_categorical_accuracy: 0.8604\n",
      "Epoch 39/50\n",
      "176/175 [==============================] - 82s 465ms/step - loss: 0.2035 - categorical_accuracy: 0.9245 - val_loss: 0.4271 - val_categorical_accuracy: 0.8571\n",
      "Epoch 40/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.2037 - categorical_accuracy: 0.9192 - val_loss: 0.5720 - val_categorical_accuracy: 0.8186\n",
      "Epoch 41/50\n",
      "176/175 [==============================] - 81s 462ms/step - loss: 0.2157 - categorical_accuracy: 0.9188 - val_loss: 0.3869 - val_categorical_accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.1974 - categorical_accuracy: 0.9272 - val_loss: 0.3757 - val_categorical_accuracy: 0.8828\n",
      "Epoch 43/50\n",
      "176/175 [==============================] - 81s 461ms/step - loss: 0.1948 - categorical_accuracy: 0.9252 - val_loss: 0.4110 - val_categorical_accuracy: 0.8636\n",
      "Epoch 44/50\n",
      "176/175 [==============================] - 82s 463ms/step - loss: 0.1937 - categorical_accuracy: 0.9249 - val_loss: 0.3867 - val_categorical_accuracy: 0.8764\n",
      "Epoch 45/50\n",
      "176/175 [==============================] - 81s 462ms/step - loss: 0.1900 - categorical_accuracy: 0.9300 - val_loss: 0.3864 - val_categorical_accuracy: 0.8716\n",
      "Epoch 46/50\n",
      "176/175 [==============================] - 82s 463ms/step - loss: 0.1895 - categorical_accuracy: 0.9266 - val_loss: 0.3938 - val_categorical_accuracy: 0.8700\n",
      "Epoch 47/50\n",
      "176/175 [==============================] - 82s 467ms/step - loss: 0.1931 - categorical_accuracy: 0.9238 - val_loss: 0.4613 - val_categorical_accuracy: 0.8620\n",
      "Epoch 48/50\n",
      "176/175 [==============================] - 83s 469ms/step - loss: 0.1843 - categorical_accuracy: 0.9322 - val_loss: 0.4322 - val_categorical_accuracy: 0.8604\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/175 [==============================] - 81s 459ms/step - loss: 0.1772 - categorical_accuracy: 0.9294 - val_loss: 0.5288 - val_categorical_accuracy: 0.8427\n",
      "Epoch 50/50\n",
      "176/175 [==============================] - 83s 472ms/step - loss: 0.1798 - categorical_accuracy: 0.9325 - val_loss: 0.4543 - val_categorical_accuracy: 0.8636\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Begin training for transfer learning\n",
    "history = model.fit_generator(\n",
    "    generator=generator_train,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    class_weight=class_weight,\n",
    "    validation_data=generator_test,\n",
    "    validation_steps=steps_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('k2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 623 images belonging to 5 classes.\n",
      "20/19 [==============================] - 7s 340ms/step\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen_test.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    " \n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    " \n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    " \n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    " \n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict_generator(\n",
    "    validation_generator,\n",
    "    steps=validation_generator.samples/validation_generator.batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8605983104776662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(ground_truth, predicted_classes, average='weighted')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
